{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMj55YDKG6ch",
    "outputId": "fc40ecc9-4756-48b1-d5c6-c169a8b453b2"
   },
   "outputs": [],
   "source": [
    "tr_path = 'deal_train.csv'  # path to training data\n",
    "tt_path = 'deal_test.csv'   # path to te|sting data\n",
    "se_path = 'season.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "k-onQd4JNA5H"
   },
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For data preprocess\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "myseed = 42069  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "FWMT3uf1NGQp"
   },
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    ''' Get device (if GPU is available, use GPU) '''\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def plot_learning_curve(loss_record, title=''):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_steps = len(loss_record['train'])\n",
    "    x_1 = range(total_steps)\n",
    "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.ylim(0.0, 1.)\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "0zlpIp9ANJRU"
   },
   "outputs": [],
   "source": [
    "tr_sz = 0\n",
    "dv_sz = 0\n",
    "class empDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 mode='train',\n",
    "                 target_only=False):\n",
    "        self.mode = mode\n",
    "        global tr_sz\n",
    "        global dv_sz\n",
    "        # Read data into numpy arrays\n",
    "        with open(path, 'r', encoding=\"Big5\") as fp:\n",
    "            data = list(csv.reader(fp))\n",
    "            data = np.array([list(map(float,i)) for i in data[1:]]).astype(float)\n",
    "        self.idx = data[:,2]\n",
    "        if not target_only:\n",
    "            feats = list(range(4,46))\n",
    "        else:\n",
    "            feats = list(range(4,46))\n",
    "        if mode == 'test':\n",
    "            data = data[:, feats]\n",
    "            self.data = torch.FloatTensor(data)\n",
    "        else:\n",
    "            target = data[:, 3]\n",
    "            data = data[:, feats]\n",
    "            \n",
    "            # Splitting training data into train & dev sets\n",
    "            if mode == 'train':\n",
    "                indices = [i for i in range(len(data)) if i % 10 < 7]\n",
    "#             elif mode == 'valid':\n",
    "#                 indices = [i for i in range(len(data)) if i % 10 == 0]\n",
    "            elif mode == 'dev':\n",
    "                indices = [i for i in range(len(data)) if i % 10 >= 7]\n",
    "            # Convert data into PyTorch tensors\n",
    "            self.data = torch.FloatTensor(data[indices])\n",
    "            self.target = torch.LongTensor(target[indices])\n",
    "        \n",
    "        self.dim = self.data.shape[1]\n",
    "        if mode == 'train':\n",
    "            tr_sz = len(self.data)\n",
    "        elif mode == 'dev':\n",
    "            dv_sz = len(self.data)\n",
    "        print('Finished reading the {} set of Dataset ({} samples found, each dim = {})'\n",
    "              .format(mode, len(self.data), self.dim))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Returns one sample at a time\n",
    "        if self.mode in ['train', 'dev', 'valid']:\n",
    "            # For training\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            # For testing (no target)\n",
    "            return self.data[index], self.idx[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the size of the dataset\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "hlhLk5t6MBX3"
   },
   "outputs": [],
   "source": [
    "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
    "    ''' Generates a dataset, then is put into a dataloader. '''\n",
    "    dataset = empDataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size,\n",
    "        shuffle=(mode == 'train'), drop_last=False,\n",
    "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "49-uXYovOAI0"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    ''' A simple fully-connected deep neural network '''\n",
    "    def __init__(self, input_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,2),\n",
    "        )\n",
    "        self.net.apply(init_weights)\n",
    "        self.criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        ''' Calculate loss '''\n",
    "        return self.criterion(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lOqcmYzMO7jB"
   },
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "def train(tr_set, dv_set, model, config, device, tr_sz, dv_sz):\n",
    "    ''' DNN training '''\n",
    "\n",
    "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
    "        model.parameters(), **config['optim_hparas'])\n",
    "    global best_acc\n",
    "    epoch = 0\n",
    "    model_path = './model.ckpt'\n",
    "    loss_record = {\"train\":[], \"dev\":[]}\n",
    "    while epoch < n_epochs:\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train()                           # set model to training mode\n",
    "        for inputs, labels in tr_set:                     # iterate through the dataloader\n",
    "            inputs, labels = inputs.to(device), labels.to(device)   # move data to device (cpu/cuda)\n",
    "            outputs = model(inputs)                     # forward pass (compute output)\n",
    "#             print(outputs, labels, inputs)\n",
    "            batch_loss = model.cal_loss(outputs, labels)  # compute loss\n",
    "            _, train_pred = torch.max(outputs, 1) \n",
    "            batch_loss.backward()                 # compute gradient (backpropagation)\n",
    "            optimizer.step()                    # update model with optimizer\n",
    "            optimizer.zero_grad()               # set gradient to zero\n",
    "            train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "            train_loss += batch_loss.item()\n",
    "        loss_record[\"train\"].append(train_loss/len(tr_set))\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dv_set:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                batch_loss = model.cal_loss(outputs, labels) \n",
    "                _, val_pred = torch.max(outputs, 1) \n",
    "\n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                val_loss += batch_loss.item()\n",
    "            loss_record[\"dev\"].append(val_loss/len(dv_set))\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, n_epochs, train_acc/tr_sz, train_loss/len(tr_set), val_acc/dv_sz, val_loss/len(dv_set)\n",
    "            ))\n",
    "            # if the model improves, save a checkpoint at this epoch\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(best_acc/dv_sz))\n",
    "\n",
    "        epoch += 1\n",
    "    print('Finished training after {} epochs'.format(epoch))\n",
    "    return val_loss, loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "NPXpdumwPjE7"
   },
   "outputs": [],
   "source": [
    "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
    "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
    "target_only = True                 # TODO: Using 40 states & 2 tested_positive features\n",
    "\n",
    "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
    "config = {\n",
    "    'n_epochs': 300,                # maximum number of epochs\n",
    "    'batch_size': 256,               # mini-batch size for dataloader\n",
    "    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)\n",
    "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
    "        'lr': 0.0001,                 # learning rate of SGD\n",
    "#         'weight_decay':0.005,\n",
    "        'momentum': 0.9              # momentum for SGD\n",
    "    },\n",
    "    'early_stop': 1000,               # early stopping epochs (the number epochs since your model's last improvement)\n",
    "    'save_path': 'models/model.pth'  # your model will be saved here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNrYBMmePLKm",
    "outputId": "fcd4f175-4f7e-4306-f33c-5f8285f11dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of Dataset (10075 samples found, each dim = 42)\n",
      "Finished reading the dev set of Dataset (4317 samples found, each dim = 42)\n",
      "Finished reading the test set of Dataset (3739 samples found, each dim = 42)\n"
     ]
    }
   ],
   "source": [
    "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
    "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
    "# va_set = prep_dataloader(tr_path, 'valid', config['batch_size'], target_only=target_only)\n",
    "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "FHylSirLP9oh"
   },
   "outputs": [],
   "source": [
    "model = Classifier(tr_set.dataset.dim).to(device)  # Construct model and move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GrEbUxazQAAZ",
    "outputId": "f4f3bd74-2d97-4275-b69f-6609976b91f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/300] Train Acc: 0.854094 Loss: 16703.432511 | Val Acc: 0.946491 loss: 6.246940\n",
      "saving model with acc 0.946\n",
      "[002/300] Train Acc: 0.943921 Loss: 0.676904 | Val Acc: 0.946491 loss: 6.910734\n",
      "[003/300] Train Acc: 0.943921 Loss: 0.662169 | Val Acc: 0.946491 loss: 6.906452\n",
      "[004/300] Train Acc: 0.943921 Loss: 0.647843 | Val Acc: 0.946491 loss: 6.892586\n",
      "[005/300] Train Acc: 0.943921 Loss: 0.634134 | Val Acc: 0.946491 loss: 6.879096\n",
      "[006/300] Train Acc: 0.943921 Loss: 0.621036 | Val Acc: 0.946491 loss: 6.866152\n",
      "[007/300] Train Acc: 0.943921 Loss: 0.608235 | Val Acc: 0.946491 loss: 6.853705\n",
      "[008/300] Train Acc: 0.943921 Loss: 0.596021 | Val Acc: 0.946491 loss: 6.841721\n",
      "[009/300] Train Acc: 0.943921 Loss: 0.584219 | Val Acc: 0.946491 loss: 6.830210\n",
      "[010/300] Train Acc: 0.943921 Loss: 0.573145 | Val Acc: 0.946491 loss: 6.819109\n",
      "[011/300] Train Acc: 0.943921 Loss: 0.562190 | Val Acc: 0.946491 loss: 6.808480\n",
      "[012/300] Train Acc: 0.943921 Loss: 0.552035 | Val Acc: 0.946491 loss: 6.798193\n",
      "[013/300] Train Acc: 0.943921 Loss: 0.541814 | Val Acc: 0.946491 loss: 6.788382\n",
      "[014/300] Train Acc: 0.943921 Loss: 0.532393 | Val Acc: 0.946491 loss: 6.778882\n",
      "[015/300] Train Acc: 0.943921 Loss: 0.523083 | Val Acc: 0.946491 loss: 6.769758\n",
      "[016/300] Train Acc: 0.943921 Loss: 0.514157 | Val Acc: 0.946491 loss: 6.761025\n",
      "[017/300] Train Acc: 0.943921 Loss: 0.505332 | Val Acc: 0.946491 loss: 6.752544\n",
      "[018/300] Train Acc: 0.943921 Loss: 0.497130 | Val Acc: 0.946491 loss: 6.744394\n",
      "[019/300] Train Acc: 0.943921 Loss: 0.489165 | Val Acc: 0.946491 loss: 6.736512\n",
      "[020/300] Train Acc: 0.943921 Loss: 0.481376 | Val Acc: 0.946491 loss: 6.728936\n",
      "[021/300] Train Acc: 0.943921 Loss: 0.474104 | Val Acc: 0.946491 loss: 6.721618\n",
      "[022/300] Train Acc: 0.943921 Loss: 0.467078 | Val Acc: 0.946491 loss: 6.714609\n",
      "[023/300] Train Acc: 0.943921 Loss: 0.459960 | Val Acc: 0.946491 loss: 6.707820\n",
      "[024/300] Train Acc: 0.943921 Loss: 0.453549 | Val Acc: 0.946491 loss: 6.701290\n",
      "[025/300] Train Acc: 0.943921 Loss: 0.447407 | Val Acc: 0.946491 loss: 6.694972\n",
      "[026/300] Train Acc: 0.943921 Loss: 0.441057 | Val Acc: 0.946491 loss: 6.688885\n",
      "[027/300] Train Acc: 0.943921 Loss: 0.434987 | Val Acc: 0.946491 loss: 6.683009\n",
      "[028/300] Train Acc: 0.943921 Loss: 0.429581 | Val Acc: 0.946491 loss: 6.677331\n",
      "[029/300] Train Acc: 0.943921 Loss: 0.423769 | Val Acc: 0.946491 loss: 6.671865\n",
      "[030/300] Train Acc: 0.943921 Loss: 0.418426 | Val Acc: 0.946491 loss: 6.666558\n",
      "[031/300] Train Acc: 0.943921 Loss: 0.412894 | Val Acc: 0.946491 loss: 6.661422\n",
      "[032/300] Train Acc: 0.943921 Loss: 0.408083 | Val Acc: 0.946491 loss: 6.656488\n",
      "[033/300] Train Acc: 0.943921 Loss: 0.403859 | Val Acc: 0.946491 loss: 6.651668\n",
      "[034/300] Train Acc: 0.943921 Loss: 0.398506 | Val Acc: 0.946491 loss: 6.647069\n",
      "[035/300] Train Acc: 0.943921 Loss: 0.394048 | Val Acc: 0.946491 loss: 6.642592\n",
      "[036/300] Train Acc: 0.943921 Loss: 0.389345 | Val Acc: 0.946491 loss: 6.638250\n",
      "[037/300] Train Acc: 0.943921 Loss: 0.385918 | Val Acc: 0.946491 loss: 6.634039\n",
      "[038/300] Train Acc: 0.943921 Loss: 0.381830 | Val Acc: 0.946491 loss: 6.629974\n",
      "[039/300] Train Acc: 0.943921 Loss: 0.377359 | Val Acc: 0.946491 loss: 6.626054\n",
      "[040/300] Train Acc: 0.943921 Loss: 0.374211 | Val Acc: 0.946491 loss: 6.622243\n",
      "[041/300] Train Acc: 0.943921 Loss: 0.369571 | Val Acc: 0.946491 loss: 6.618537\n",
      "[042/300] Train Acc: 0.943921 Loss: 0.366546 | Val Acc: 0.946491 loss: 6.614963\n",
      "[043/300] Train Acc: 0.943921 Loss: 0.363242 | Val Acc: 0.946491 loss: 6.611493\n",
      "[044/300] Train Acc: 0.943921 Loss: 0.360049 | Val Acc: 0.946491 loss: 6.608140\n",
      "[045/300] Train Acc: 0.943921 Loss: 0.355840 | Val Acc: 0.946491 loss: 6.604883\n",
      "[046/300] Train Acc: 0.943921 Loss: 0.352839 | Val Acc: 0.946491 loss: 6.601709\n",
      "[047/300] Train Acc: 0.943921 Loss: 0.349884 | Val Acc: 0.946491 loss: 6.598642\n",
      "[048/300] Train Acc: 0.943921 Loss: 0.346743 | Val Acc: 0.946491 loss: 6.595650\n",
      "[049/300] Train Acc: 0.943921 Loss: 0.343779 | Val Acc: 0.946491 loss: 6.592746\n",
      "[050/300] Train Acc: 0.943921 Loss: 0.342011 | Val Acc: 0.946491 loss: 6.589934\n",
      "[051/300] Train Acc: 0.943921 Loss: 0.338463 | Val Acc: 0.946491 loss: 6.587225\n",
      "[052/300] Train Acc: 0.943921 Loss: 0.336793 | Val Acc: 0.946491 loss: 6.584579\n",
      "[053/300] Train Acc: 0.943921 Loss: 0.333226 | Val Acc: 0.946491 loss: 6.582014\n",
      "[054/300] Train Acc: 0.943921 Loss: 0.330716 | Val Acc: 0.946491 loss: 6.579507\n",
      "[055/300] Train Acc: 0.943921 Loss: 0.328272 | Val Acc: 0.946491 loss: 6.577068\n",
      "[056/300] Train Acc: 0.943921 Loss: 0.326335 | Val Acc: 0.946491 loss: 6.574685\n",
      "[057/300] Train Acc: 0.943921 Loss: 0.323545 | Val Acc: 0.946491 loss: 6.572392\n",
      "[058/300] Train Acc: 0.943921 Loss: 0.322234 | Val Acc: 0.946491 loss: 6.570152\n",
      "[059/300] Train Acc: 0.943921 Loss: 0.318897 | Val Acc: 0.946491 loss: 6.567999\n",
      "[060/300] Train Acc: 0.943921 Loss: 0.316366 | Val Acc: 0.946491 loss: 6.565865\n",
      "[061/300] Train Acc: 0.943921 Loss: 0.315661 | Val Acc: 0.946491 loss: 6.563781\n",
      "[062/300] Train Acc: 0.943921 Loss: 0.312962 | Val Acc: 0.946491 loss: 6.561773\n",
      "[063/300] Train Acc: 0.943921 Loss: 0.311236 | Val Acc: 0.946491 loss: 6.559823\n",
      "[064/300] Train Acc: 0.943921 Loss: 0.308390 | Val Acc: 0.946491 loss: 6.557926\n",
      "[065/300] Train Acc: 0.943921 Loss: 0.307155 | Val Acc: 0.946491 loss: 6.556059\n",
      "[066/300] Train Acc: 0.943921 Loss: 0.305627 | Val Acc: 0.946491 loss: 6.554246\n",
      "[067/300] Train Acc: 0.943921 Loss: 0.303869 | Val Acc: 0.946491 loss: 6.552484\n",
      "[068/300] Train Acc: 0.943921 Loss: 0.302155 | Val Acc: 0.946491 loss: 6.550772\n",
      "[069/300] Train Acc: 0.943921 Loss: 0.300036 | Val Acc: 0.946491 loss: 6.549091\n",
      "[070/300] Train Acc: 0.943921 Loss: 0.298896 | Val Acc: 0.946491 loss: 6.547454\n",
      "[071/300] Train Acc: 0.943921 Loss: 0.296809 | Val Acc: 0.946491 loss: 6.545864\n",
      "[072/300] Train Acc: 0.943921 Loss: 0.296011 | Val Acc: 0.946491 loss: 6.544318\n",
      "[073/300] Train Acc: 0.943921 Loss: 0.293746 | Val Acc: 0.946491 loss: 6.542793\n",
      "[074/300] Train Acc: 0.943921 Loss: 0.291758 | Val Acc: 0.946491 loss: 6.541300\n",
      "[075/300] Train Acc: 0.943921 Loss: 0.290562 | Val Acc: 0.946491 loss: 6.539863\n",
      "[076/300] Train Acc: 0.943921 Loss: 0.289669 | Val Acc: 0.946491 loss: 6.538458\n",
      "[077/300] Train Acc: 0.943921 Loss: 0.289350 | Val Acc: 0.946491 loss: 6.537074\n",
      "[078/300] Train Acc: 0.943921 Loss: 0.287498 | Val Acc: 0.946491 loss: 6.535737\n",
      "[079/300] Train Acc: 0.943921 Loss: 0.286465 | Val Acc: 0.946491 loss: 6.534429\n",
      "[080/300] Train Acc: 0.943921 Loss: 0.284131 | Val Acc: 0.946491 loss: 6.533162\n",
      "[081/300] Train Acc: 0.943921 Loss: 0.283692 | Val Acc: 0.946491 loss: 6.531912\n",
      "[082/300] Train Acc: 0.943921 Loss: 0.282756 | Val Acc: 0.946491 loss: 6.530688\n",
      "[083/300] Train Acc: 0.943921 Loss: 0.280761 | Val Acc: 0.946491 loss: 6.529494\n",
      "[084/300] Train Acc: 0.943921 Loss: 0.280151 | Val Acc: 0.946491 loss: 6.528323\n",
      "[085/300] Train Acc: 0.943921 Loss: 0.277916 | Val Acc: 0.946491 loss: 6.527201\n",
      "[086/300] Train Acc: 0.943921 Loss: 0.277917 | Val Acc: 0.946491 loss: 6.526081\n",
      "[087/300] Train Acc: 0.943921 Loss: 0.276285 | Val Acc: 0.946491 loss: 6.524985\n",
      "[088/300] Train Acc: 0.943921 Loss: 0.275783 | Val Acc: 0.946491 loss: 6.523927\n",
      "[089/300] Train Acc: 0.943921 Loss: 0.273880 | Val Acc: 0.946491 loss: 6.522879\n",
      "[090/300] Train Acc: 0.943921 Loss: 0.273750 | Val Acc: 0.946491 loss: 6.521856\n",
      "[091/300] Train Acc: 0.943921 Loss: 0.272478 | Val Acc: 0.946491 loss: 6.520872\n",
      "[092/300] Train Acc: 0.943921 Loss: 0.271805 | Val Acc: 0.946491 loss: 6.519887\n",
      "[093/300] Train Acc: 0.943921 Loss: 0.270286 | Val Acc: 0.946491 loss: 6.518943\n",
      "[094/300] Train Acc: 0.943921 Loss: 0.268491 | Val Acc: 0.946491 loss: 6.518012\n",
      "[095/300] Train Acc: 0.943921 Loss: 0.267873 | Val Acc: 0.946491 loss: 6.517082\n",
      "[096/300] Train Acc: 0.943921 Loss: 0.268411 | Val Acc: 0.946491 loss: 6.516186\n",
      "[097/300] Train Acc: 0.943921 Loss: 0.266343 | Val Acc: 0.946491 loss: 6.515316\n",
      "[098/300] Train Acc: 0.943921 Loss: 0.264962 | Val Acc: 0.946491 loss: 6.514468\n",
      "[099/300] Train Acc: 0.943921 Loss: 0.264989 | Val Acc: 0.946491 loss: 6.513618\n",
      "[100/300] Train Acc: 0.943921 Loss: 0.263907 | Val Acc: 0.946491 loss: 6.512790\n",
      "[101/300] Train Acc: 0.943921 Loss: 0.263406 | Val Acc: 0.946491 loss: 6.511988\n",
      "[102/300] Train Acc: 0.943921 Loss: 0.262627 | Val Acc: 0.946491 loss: 6.511201\n",
      "[103/300] Train Acc: 0.943921 Loss: 0.260926 | Val Acc: 0.946491 loss: 6.510427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104/300] Train Acc: 0.943921 Loss: 0.261421 | Val Acc: 0.946491 loss: 6.509657\n",
      "[105/300] Train Acc: 0.943921 Loss: 0.260380 | Val Acc: 0.946491 loss: 6.508922\n",
      "[106/300] Train Acc: 0.943921 Loss: 0.259973 | Val Acc: 0.946491 loss: 6.508189\n",
      "[107/300] Train Acc: 0.943921 Loss: 0.259269 | Val Acc: 0.946491 loss: 6.507487\n",
      "[108/300] Train Acc: 0.943921 Loss: 0.258586 | Val Acc: 0.946491 loss: 6.506787\n",
      "[109/300] Train Acc: 0.943921 Loss: 0.258846 | Val Acc: 0.946491 loss: 6.506100\n",
      "[110/300] Train Acc: 0.943921 Loss: 0.256320 | Val Acc: 0.946491 loss: 6.505437\n",
      "[111/300] Train Acc: 0.943921 Loss: 0.255671 | Val Acc: 0.946491 loss: 6.504774\n",
      "[112/300] Train Acc: 0.943921 Loss: 0.255657 | Val Acc: 0.946491 loss: 6.504128\n",
      "[113/300] Train Acc: 0.943921 Loss: 0.254377 | Val Acc: 0.946491 loss: 6.503492\n",
      "[114/300] Train Acc: 0.943921 Loss: 0.255055 | Val Acc: 0.946491 loss: 6.502873\n",
      "[115/300] Train Acc: 0.943921 Loss: 0.253505 | Val Acc: 0.946491 loss: 6.502268\n",
      "[116/300] Train Acc: 0.943921 Loss: 0.253875 | Val Acc: 0.946491 loss: 6.501671\n",
      "[117/300] Train Acc: 0.943921 Loss: 0.253623 | Val Acc: 0.946491 loss: 6.501087\n",
      "[118/300] Train Acc: 0.943921 Loss: 0.252424 | Val Acc: 0.946491 loss: 6.500508\n",
      "[119/300] Train Acc: 0.943921 Loss: 0.251547 | Val Acc: 0.946491 loss: 6.499949\n",
      "[120/300] Train Acc: 0.943921 Loss: 0.250678 | Val Acc: 0.946491 loss: 6.499402\n",
      "[121/300] Train Acc: 0.943921 Loss: 0.250473 | Val Acc: 0.946491 loss: 6.498847\n",
      "[122/300] Train Acc: 0.943921 Loss: 0.249619 | Val Acc: 0.946491 loss: 6.498317\n",
      "[123/300] Train Acc: 0.943921 Loss: 0.249104 | Val Acc: 0.946491 loss: 6.497790\n",
      "[124/300] Train Acc: 0.943921 Loss: 0.249256 | Val Acc: 0.946491 loss: 6.497272\n",
      "[125/300] Train Acc: 0.943921 Loss: 0.248762 | Val Acc: 0.946491 loss: 6.496772\n",
      "[126/300] Train Acc: 0.943921 Loss: 0.247947 | Val Acc: 0.946491 loss: 6.496281\n",
      "[127/300] Train Acc: 0.943921 Loss: 0.247113 | Val Acc: 0.946491 loss: 6.495799\n",
      "[128/300] Train Acc: 0.943921 Loss: 0.246337 | Val Acc: 0.946491 loss: 6.495316\n",
      "[129/300] Train Acc: 0.943921 Loss: 0.246877 | Val Acc: 0.946491 loss: 6.494841\n",
      "[130/300] Train Acc: 0.943921 Loss: 0.245083 | Val Acc: 0.946491 loss: 6.494390\n",
      "[131/300] Train Acc: 0.943921 Loss: 0.245959 | Val Acc: 0.946491 loss: 6.493926\n",
      "[132/300] Train Acc: 0.943921 Loss: 0.244462 | Val Acc: 0.946491 loss: 6.493484\n",
      "[133/300] Train Acc: 0.943921 Loss: 0.244107 | Val Acc: 0.946491 loss: 6.493038\n",
      "[134/300] Train Acc: 0.943921 Loss: 0.245038 | Val Acc: 0.946491 loss: 6.492617\n",
      "[135/300] Train Acc: 0.943921 Loss: 0.244608 | Val Acc: 0.946491 loss: 6.492194\n",
      "[136/300] Train Acc: 0.943921 Loss: 0.245255 | Val Acc: 0.946491 loss: 6.491781\n",
      "[137/300] Train Acc: 0.943921 Loss: 0.242469 | Val Acc: 0.946491 loss: 6.491378\n",
      "[138/300] Train Acc: 0.943921 Loss: 0.243107 | Val Acc: 0.946491 loss: 6.490973\n",
      "[139/300] Train Acc: 0.943921 Loss: 0.243068 | Val Acc: 0.946491 loss: 6.490581\n",
      "[140/300] Train Acc: 0.943921 Loss: 0.240967 | Val Acc: 0.946491 loss: 6.490199\n",
      "[141/300] Train Acc: 0.943921 Loss: 0.241238 | Val Acc: 0.946491 loss: 6.489818\n",
      "[142/300] Train Acc: 0.943921 Loss: 0.240918 | Val Acc: 0.946491 loss: 6.489445\n",
      "[143/300] Train Acc: 0.943921 Loss: 0.239862 | Val Acc: 0.946491 loss: 6.489074\n",
      "[144/300] Train Acc: 0.943921 Loss: 0.240204 | Val Acc: 0.946491 loss: 6.488708\n",
      "[145/300] Train Acc: 0.943921 Loss: 0.241933 | Val Acc: 0.946491 loss: 6.488349\n",
      "[146/300] Train Acc: 0.943921 Loss: 0.238814 | Val Acc: 0.946491 loss: 6.488008\n",
      "[147/300] Train Acc: 0.943921 Loss: 0.238829 | Val Acc: 0.946491 loss: 6.487663\n",
      "[148/300] Train Acc: 0.943921 Loss: 0.238145 | Val Acc: 0.946491 loss: 6.487316\n",
      "[149/300] Train Acc: 0.943921 Loss: 0.238877 | Val Acc: 0.946491 loss: 6.486980\n",
      "[150/300] Train Acc: 0.943921 Loss: 0.238557 | Val Acc: 0.946491 loss: 6.486660\n",
      "[151/300] Train Acc: 0.943921 Loss: 0.238602 | Val Acc: 0.946491 loss: 6.486334\n",
      "[152/300] Train Acc: 0.943921 Loss: 0.237582 | Val Acc: 0.946491 loss: 6.486018\n",
      "[153/300] Train Acc: 0.943921 Loss: 0.237280 | Val Acc: 0.946491 loss: 6.485708\n",
      "[154/300] Train Acc: 0.943921 Loss: 0.237697 | Val Acc: 0.946491 loss: 6.485402\n",
      "[155/300] Train Acc: 0.943921 Loss: 0.236688 | Val Acc: 0.946491 loss: 6.485099\n",
      "[156/300] Train Acc: 0.943921 Loss: 0.237839 | Val Acc: 0.946491 loss: 6.484797\n",
      "[157/300] Train Acc: 0.943921 Loss: 0.236816 | Val Acc: 0.946491 loss: 6.484511\n",
      "[158/300] Train Acc: 0.943921 Loss: 0.235838 | Val Acc: 0.946491 loss: 6.484229\n",
      "[159/300] Train Acc: 0.943921 Loss: 0.235564 | Val Acc: 0.946491 loss: 6.483947\n",
      "[160/300] Train Acc: 0.943921 Loss: 0.236726 | Val Acc: 0.946491 loss: 6.483661\n",
      "[161/300] Train Acc: 0.943921 Loss: 0.237211 | Val Acc: 0.946491 loss: 6.483390\n",
      "[162/300] Train Acc: 0.943921 Loss: 0.235115 | Val Acc: 0.946491 loss: 6.483122\n",
      "[163/300] Train Acc: 0.943921 Loss: 0.235245 | Val Acc: 0.946491 loss: 6.482858\n",
      "[164/300] Train Acc: 0.943921 Loss: 0.233528 | Val Acc: 0.946491 loss: 6.482601\n",
      "[165/300] Train Acc: 0.943921 Loss: 0.233643 | Val Acc: 0.946491 loss: 6.482339\n",
      "[166/300] Train Acc: 0.943921 Loss: 0.234499 | Val Acc: 0.946491 loss: 6.482085\n",
      "[167/300] Train Acc: 0.943921 Loss: 0.233131 | Val Acc: 0.946491 loss: 6.481836\n",
      "[168/300] Train Acc: 0.943921 Loss: 0.234022 | Val Acc: 0.946491 loss: 6.481587\n",
      "[169/300] Train Acc: 0.943921 Loss: 0.233787 | Val Acc: 0.946491 loss: 6.481350\n",
      "[170/300] Train Acc: 0.943921 Loss: 0.233190 | Val Acc: 0.946491 loss: 6.481113\n",
      "[171/300] Train Acc: 0.943921 Loss: 0.232201 | Val Acc: 0.946491 loss: 6.480875\n",
      "[172/300] Train Acc: 0.943921 Loss: 0.233092 | Val Acc: 0.946491 loss: 6.480644\n",
      "[173/300] Train Acc: 0.943921 Loss: 0.231775 | Val Acc: 0.946491 loss: 6.480414\n",
      "[174/300] Train Acc: 0.943921 Loss: 0.233051 | Val Acc: 0.946491 loss: 6.480187\n",
      "[175/300] Train Acc: 0.943921 Loss: 0.232092 | Val Acc: 0.946491 loss: 6.479970\n",
      "[176/300] Train Acc: 0.943921 Loss: 0.232238 | Val Acc: 0.946491 loss: 6.479750\n",
      "[177/300] Train Acc: 0.943921 Loss: 0.231301 | Val Acc: 0.946491 loss: 6.479542\n",
      "[178/300] Train Acc: 0.943921 Loss: 0.231851 | Val Acc: 0.946491 loss: 6.479323\n",
      "[179/300] Train Acc: 0.943921 Loss: 0.230479 | Val Acc: 0.946491 loss: 6.479120\n",
      "[180/300] Train Acc: 0.943921 Loss: 0.230679 | Val Acc: 0.946491 loss: 6.478910\n",
      "[181/300] Train Acc: 0.943921 Loss: 0.229745 | Val Acc: 0.946491 loss: 6.478709\n",
      "[182/300] Train Acc: 0.943921 Loss: 0.231068 | Val Acc: 0.946491 loss: 6.478506\n",
      "[183/300] Train Acc: 0.943921 Loss: 0.230881 | Val Acc: 0.946491 loss: 6.478308\n",
      "[184/300] Train Acc: 0.943921 Loss: 0.231837 | Val Acc: 0.946491 loss: 6.478121\n",
      "[185/300] Train Acc: 0.943921 Loss: 0.229352 | Val Acc: 0.946491 loss: 6.477930\n",
      "[186/300] Train Acc: 0.943921 Loss: 0.229572 | Val Acc: 0.946491 loss: 6.477744\n",
      "[187/300] Train Acc: 0.943921 Loss: 0.229374 | Val Acc: 0.946491 loss: 6.477558\n",
      "[188/300] Train Acc: 0.943921 Loss: 0.229602 | Val Acc: 0.946491 loss: 6.477371\n",
      "[189/300] Train Acc: 0.943921 Loss: 0.230196 | Val Acc: 0.946491 loss: 6.477195\n",
      "[190/300] Train Acc: 0.943921 Loss: 0.231184 | Val Acc: 0.946491 loss: 6.477017\n",
      "[191/300] Train Acc: 0.943921 Loss: 0.228691 | Val Acc: 0.946491 loss: 6.476846\n",
      "[192/300] Train Acc: 0.943921 Loss: 0.227772 | Val Acc: 0.946491 loss: 6.476676\n",
      "[193/300] Train Acc: 0.943921 Loss: 0.228767 | Val Acc: 0.946491 loss: 6.476500\n",
      "[194/300] Train Acc: 0.943921 Loss: 0.227812 | Val Acc: 0.946491 loss: 6.476332\n",
      "[195/300] Train Acc: 0.943921 Loss: 0.229223 | Val Acc: 0.946491 loss: 6.476165\n",
      "[196/300] Train Acc: 0.943921 Loss: 0.227904 | Val Acc: 0.946491 loss: 6.476002\n",
      "[197/300] Train Acc: 0.943921 Loss: 0.228139 | Val Acc: 0.946491 loss: 6.475841\n",
      "[198/300] Train Acc: 0.943921 Loss: 0.227987 | Val Acc: 0.946491 loss: 6.475682\n",
      "[199/300] Train Acc: 0.943921 Loss: 0.228620 | Val Acc: 0.946491 loss: 6.475527\n",
      "[200/300] Train Acc: 0.943921 Loss: 0.226501 | Val Acc: 0.946491 loss: 6.475376\n",
      "[201/300] Train Acc: 0.943921 Loss: 0.227547 | Val Acc: 0.946491 loss: 6.475221\n",
      "[202/300] Train Acc: 0.943921 Loss: 0.228974 | Val Acc: 0.946491 loss: 6.475067\n",
      "[203/300] Train Acc: 0.943921 Loss: 0.228031 | Val Acc: 0.946491 loss: 6.474928\n",
      "[204/300] Train Acc: 0.943921 Loss: 0.225929 | Val Acc: 0.946491 loss: 6.474782\n",
      "[205/300] Train Acc: 0.943921 Loss: 0.226578 | Val Acc: 0.946491 loss: 6.474634\n",
      "[206/300] Train Acc: 0.943921 Loss: 0.227248 | Val Acc: 0.946491 loss: 6.474495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207/300] Train Acc: 0.943921 Loss: 0.225912 | Val Acc: 0.946491 loss: 6.474356\n",
      "[208/300] Train Acc: 0.943921 Loss: 0.226573 | Val Acc: 0.946491 loss: 6.474209\n",
      "[209/300] Train Acc: 0.943921 Loss: 0.225270 | Val Acc: 0.946491 loss: 6.474075\n",
      "[210/300] Train Acc: 0.943921 Loss: 0.225898 | Val Acc: 0.946491 loss: 6.473939\n",
      "[211/300] Train Acc: 0.943921 Loss: 0.225409 | Val Acc: 0.946491 loss: 6.473808\n",
      "[212/300] Train Acc: 0.943921 Loss: 0.226460 | Val Acc: 0.946491 loss: 6.473674\n",
      "[213/300] Train Acc: 0.943921 Loss: 0.225160 | Val Acc: 0.946491 loss: 6.473547\n",
      "[214/300] Train Acc: 0.943921 Loss: 0.225019 | Val Acc: 0.946491 loss: 6.473422\n",
      "[215/300] Train Acc: 0.943921 Loss: 0.224899 | Val Acc: 0.946491 loss: 6.473294\n",
      "[216/300] Train Acc: 0.943921 Loss: 0.225580 | Val Acc: 0.946491 loss: 6.473166\n",
      "[217/300] Train Acc: 0.943921 Loss: 0.225447 | Val Acc: 0.946491 loss: 6.473039\n",
      "[218/300] Train Acc: 0.943921 Loss: 0.224546 | Val Acc: 0.946491 loss: 6.472924\n",
      "[219/300] Train Acc: 0.943921 Loss: 0.224835 | Val Acc: 0.946491 loss: 6.472807\n",
      "[220/300] Train Acc: 0.943921 Loss: 0.223936 | Val Acc: 0.946491 loss: 6.472681\n",
      "[221/300] Train Acc: 0.943921 Loss: 0.223787 | Val Acc: 0.946491 loss: 6.472566\n",
      "[222/300] Train Acc: 0.943921 Loss: 0.225308 | Val Acc: 0.946491 loss: 6.472451\n",
      "[223/300] Train Acc: 0.943921 Loss: 0.224815 | Val Acc: 0.946491 loss: 6.472341\n",
      "[224/300] Train Acc: 0.943921 Loss: 0.225925 | Val Acc: 0.946491 loss: 6.472222\n",
      "[225/300] Train Acc: 0.943921 Loss: 0.225416 | Val Acc: 0.946491 loss: 6.472116\n",
      "[226/300] Train Acc: 0.943921 Loss: 0.224101 | Val Acc: 0.946491 loss: 6.472007\n",
      "[227/300] Train Acc: 0.943921 Loss: 0.225216 | Val Acc: 0.946491 loss: 6.471902\n",
      "[228/300] Train Acc: 0.943921 Loss: 0.223493 | Val Acc: 0.946491 loss: 6.471796\n",
      "[229/300] Train Acc: 0.943921 Loss: 0.223393 | Val Acc: 0.946491 loss: 6.471690\n",
      "[230/300] Train Acc: 0.943921 Loss: 0.225330 | Val Acc: 0.946491 loss: 6.471589\n",
      "[231/300] Train Acc: 0.943921 Loss: 0.224405 | Val Acc: 0.946491 loss: 6.471486\n",
      "[232/300] Train Acc: 0.943921 Loss: 0.223920 | Val Acc: 0.946491 loss: 6.471385\n",
      "[233/300] Train Acc: 0.943921 Loss: 0.224645 | Val Acc: 0.946491 loss: 6.471285\n",
      "[234/300] Train Acc: 0.943921 Loss: 0.221670 | Val Acc: 0.946491 loss: 6.471190\n",
      "[235/300] Train Acc: 0.943921 Loss: 0.225282 | Val Acc: 0.946491 loss: 6.471089\n",
      "[236/300] Train Acc: 0.943921 Loss: 0.222733 | Val Acc: 0.946491 loss: 6.470996\n",
      "[237/300] Train Acc: 0.943921 Loss: 0.225929 | Val Acc: 0.946491 loss: 6.470901\n",
      "[238/300] Train Acc: 0.943921 Loss: 0.223379 | Val Acc: 0.946491 loss: 6.470807\n",
      "[239/300] Train Acc: 0.943921 Loss: 0.222881 | Val Acc: 0.946491 loss: 6.470714\n",
      "[240/300] Train Acc: 0.943921 Loss: 0.221971 | Val Acc: 0.946491 loss: 6.470623\n",
      "[241/300] Train Acc: 0.943921 Loss: 0.222709 | Val Acc: 0.946491 loss: 6.470534\n",
      "[242/300] Train Acc: 0.943921 Loss: 0.223452 | Val Acc: 0.946491 loss: 6.470444\n",
      "[243/300] Train Acc: 0.943921 Loss: 0.221715 | Val Acc: 0.946491 loss: 6.470355\n",
      "[244/300] Train Acc: 0.943921 Loss: 0.223702 | Val Acc: 0.946491 loss: 6.470269\n",
      "[245/300] Train Acc: 0.943921 Loss: 0.221967 | Val Acc: 0.946491 loss: 6.470183\n",
      "[246/300] Train Acc: 0.943921 Loss: 0.223545 | Val Acc: 0.946491 loss: 6.470098\n",
      "[247/300] Train Acc: 0.943921 Loss: 0.225115 | Val Acc: 0.946491 loss: 6.470011\n",
      "[248/300] Train Acc: 0.943921 Loss: 0.224227 | Val Acc: 0.946491 loss: 6.469929\n",
      "[249/300] Train Acc: 0.943921 Loss: 0.222489 | Val Acc: 0.946491 loss: 6.469856\n",
      "[250/300] Train Acc: 0.943921 Loss: 0.222415 | Val Acc: 0.946491 loss: 6.469773\n",
      "[251/300] Train Acc: 0.943921 Loss: 0.222340 | Val Acc: 0.946491 loss: 6.469692\n",
      "[252/300] Train Acc: 0.943921 Loss: 0.222669 | Val Acc: 0.946491 loss: 6.469617\n",
      "[253/300] Train Acc: 0.943921 Loss: 0.223031 | Val Acc: 0.946491 loss: 6.469539\n",
      "[254/300] Train Acc: 0.943921 Loss: 0.222125 | Val Acc: 0.946491 loss: 6.469462\n",
      "[255/300] Train Acc: 0.943921 Loss: 0.223311 | Val Acc: 0.946491 loss: 6.469387\n",
      "[256/300] Train Acc: 0.943921 Loss: 0.221567 | Val Acc: 0.946491 loss: 6.469313\n",
      "[257/300] Train Acc: 0.943921 Loss: 0.223176 | Val Acc: 0.946491 loss: 6.469236\n",
      "[258/300] Train Acc: 0.943921 Loss: 0.222690 | Val Acc: 0.946491 loss: 6.469168\n",
      "[259/300] Train Acc: 0.943921 Loss: 0.221364 | Val Acc: 0.946491 loss: 6.469092\n",
      "[260/300] Train Acc: 0.943921 Loss: 0.221670 | Val Acc: 0.946491 loss: 6.469019\n",
      "[261/300] Train Acc: 0.943921 Loss: 0.221652 | Val Acc: 0.946491 loss: 6.468951\n",
      "[262/300] Train Acc: 0.943921 Loss: 0.219885 | Val Acc: 0.946491 loss: 6.468883\n",
      "[263/300] Train Acc: 0.943921 Loss: 0.221523 | Val Acc: 0.946491 loss: 6.468810\n",
      "[264/300] Train Acc: 0.943921 Loss: 0.220192 | Val Acc: 0.946491 loss: 6.468741\n",
      "[265/300] Train Acc: 0.943921 Loss: 0.223933 | Val Acc: 0.946491 loss: 6.468676\n",
      "[266/300] Train Acc: 0.943921 Loss: 0.220066 | Val Acc: 0.946491 loss: 6.468612\n",
      "[267/300] Train Acc: 0.943921 Loss: 0.222530 | Val Acc: 0.946491 loss: 6.468544\n",
      "[268/300] Train Acc: 0.943921 Loss: 0.219928 | Val Acc: 0.946491 loss: 6.468478\n",
      "[269/300] Train Acc: 0.943921 Loss: 0.221156 | Val Acc: 0.946491 loss: 6.468413\n",
      "[270/300] Train Acc: 0.943921 Loss: 0.221521 | Val Acc: 0.946491 loss: 6.468350\n",
      "[271/300] Train Acc: 0.943921 Loss: 0.220614 | Val Acc: 0.946491 loss: 6.468288\n",
      "[272/300] Train Acc: 0.943921 Loss: 0.220967 | Val Acc: 0.946491 loss: 6.468223\n",
      "[273/300] Train Acc: 0.943921 Loss: 0.222203 | Val Acc: 0.946491 loss: 6.468164\n",
      "[274/300] Train Acc: 0.943921 Loss: 0.220445 | Val Acc: 0.946491 loss: 6.468101\n",
      "[275/300] Train Acc: 0.943921 Loss: 0.219962 | Val Acc: 0.946491 loss: 6.468045\n",
      "[276/300] Train Acc: 0.943921 Loss: 0.220334 | Val Acc: 0.946491 loss: 6.467984\n",
      "[277/300] Train Acc: 0.943921 Loss: 0.221988 | Val Acc: 0.946491 loss: 6.467926\n",
      "[278/300] Train Acc: 0.943921 Loss: 0.221067 | Val Acc: 0.946491 loss: 6.467867\n",
      "[279/300] Train Acc: 0.943921 Loss: 0.221458 | Val Acc: 0.946491 loss: 6.467809\n",
      "[280/300] Train Acc: 0.943921 Loss: 0.221407 | Val Acc: 0.946491 loss: 6.467755\n",
      "[281/300] Train Acc: 0.943921 Loss: 0.220072 | Val Acc: 0.946491 loss: 6.467700\n",
      "[282/300] Train Acc: 0.943921 Loss: 0.219590 | Val Acc: 0.946491 loss: 6.467648\n",
      "[283/300] Train Acc: 0.943921 Loss: 0.219110 | Val Acc: 0.946491 loss: 6.467589\n",
      "[284/300] Train Acc: 0.943921 Loss: 0.220348 | Val Acc: 0.946491 loss: 6.467535\n",
      "[285/300] Train Acc: 0.943921 Loss: 0.219438 | Val Acc: 0.946491 loss: 6.467476\n",
      "[286/300] Train Acc: 0.943921 Loss: 0.220235 | Val Acc: 0.946491 loss: 6.467425\n",
      "[287/300] Train Acc: 0.943921 Loss: 0.222357 | Val Acc: 0.946491 loss: 6.467373\n",
      "[288/300] Train Acc: 0.943921 Loss: 0.220156 | Val Acc: 0.946491 loss: 6.467325\n",
      "[289/300] Train Acc: 0.943921 Loss: 0.220110 | Val Acc: 0.946491 loss: 6.467270\n",
      "[290/300] Train Acc: 0.943921 Loss: 0.220496 | Val Acc: 0.946491 loss: 6.467222\n",
      "[291/300] Train Acc: 0.943921 Loss: 0.220004 | Val Acc: 0.946491 loss: 6.467171\n",
      "[292/300] Train Acc: 0.943921 Loss: 0.219541 | Val Acc: 0.946491 loss: 6.467123\n",
      "[293/300] Train Acc: 0.943921 Loss: 0.219914 | Val Acc: 0.946491 loss: 6.467073\n",
      "[294/300] Train Acc: 0.943921 Loss: 0.218586 | Val Acc: 0.946491 loss: 6.467024\n",
      "[295/300] Train Acc: 0.943921 Loss: 0.219842 | Val Acc: 0.946491 loss: 6.466972\n",
      "[296/300] Train Acc: 0.943921 Loss: 0.221967 | Val Acc: 0.946491 loss: 6.466929\n",
      "[297/300] Train Acc: 0.943921 Loss: 0.219322 | Val Acc: 0.946491 loss: 6.466885\n",
      "[298/300] Train Acc: 0.943921 Loss: 0.219715 | Val Acc: 0.946491 loss: 6.466832\n",
      "[299/300] Train Acc: 0.943921 Loss: 0.220542 | Val Acc: 0.946491 loss: 6.466789\n",
      "[300/300] Train Acc: 0.943921 Loss: 0.223114 | Val Acc: 0.946491 loss: 6.466744\n",
      "Finished training after 300 epochs\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device, tr_sz, dv_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "hsNO9nnXQBvP",
    "outputId": "1626def6-94c7-4a87-9447-d939f827c8eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.246939529390896, 6.910734323894276, 6.906452308682835, 6.892585919183843, 6.8790960697566765, 6.866151834235472, 6.853705486830543, 6.841721376952003, 6.830209626870997, 6.819108941975762, 6.808480325867148, 6.798192599240472, 6.788381611599641, 6.778882356250987, 6.7697579124394585, 6.761025370920406, 6.752543686067357, 6.7443943584666535, 6.736511957996032, 6.7289360393496125, 6.721618124667336, 6.714608506244772, 6.707820448805304, 6.701290277873769, 6.694971749011208, 6.688885278561536, 6.683008947793176, 6.67733100231956, 6.6718650197281555, 6.666557820404277, 6.661422096631107, 6.656487938235788, 6.651668390806983, 6.647068823085112, 6.642592161893845, 6.638250370236004, 6.634038925170898, 6.629974232000463, 6.626053757527295, 6.622242654071135, 6.618536617826013, 6.614962856559193, 6.611493117669049, 6.6081396166016075, 6.60488319922896, 6.6017089521183685, 6.598641974084518, 6.595650320543962, 6.59274560213089, 6.589934179011514, 6.587224653538535, 6.584579190787147, 6.582014059319215, 6.579506770652883, 6.577067552243962, 6.57468453575583, 6.572391911464579, 6.570151542916017, 6.567998986033833, 6.565864666419871, 6.563780859989278, 6.56177306876463, 6.559822815306046, 6.557926418150172, 6.556059334207983, 6.5542464449125175, 6.552484168725855, 6.550771702738369, 6.549090881558025, 6.547453839989269, 6.545864180606954, 6.544317815233679, 6.542792844421723, 6.5412999076001785, 6.539862524937181, 6.538457979174221, 6.537073604324284, 6.535737035029075, 6.5344289497417565, 6.533161860178499, 6.531911536174662, 6.5306876924108055, 6.529494067325311, 6.52832285621587, 6.527201409725582, 6.5260812640190125, 6.524984523653984, 6.5239274195011925, 6.522878504851285, 6.521855535752633, 6.520871928509544, 6.519887394764844, 6.518942734774421, 6.518011726877269, 6.517082259935491, 6.5161863311248664, 6.515315901707201, 6.5144680589437485, 6.513618091450018, 6.512789930490887, 6.5119880902416565, 6.51120073129149, 6.510427190976984, 6.509656508179272, 6.508922412991524, 6.508188775357078, 6.507487044614904, 6.506786793470383, 6.506100206690676, 6.505437055054833, 6.5047740042209625, 6.504128481535351, 6.50349187938606, 6.502872784348095, 6.502268242485383, 6.5016706051195365, 6.5010874578181435, 6.500508230398683, 6.4999491099049065, 6.499402016401291, 6.49884666327168, 6.498316563227597, 6.497789911487523, 6.497272242518032, 6.496771793155109, 6.496280794634538, 6.495799275882104, 6.495316362556289, 6.494841266204329, 6.494390250128858, 6.493925810736768, 6.49348392030772, 6.493037562159931, 6.492617002304862, 6.49219410033787, 6.491781105889993, 6.491377616629881, 6.49097325433703, 6.490580926046652, 6.490198966334848, 6.489817613187959, 6.489444522296681, 6.489073609604555, 6.488707960528486, 6.488348994184943, 6.488008269492318, 6.487662972772823, 6.4873159475186295, 6.48698013845612, 6.486659935292075, 6.486334443092346, 6.486018483253086, 6.485708296298981, 6.485401977511013, 6.48509904654587, 6.484797027181177, 6.484511496389613, 6.484229038743412, 6.483946656479555, 6.483660711961634, 6.483389628284118, 6.4831222795388275, 6.4828581827528335, 6.48260073363781, 6.48233935149277, 6.482084593352149, 6.481835988514564, 6.481586593915434, 6.4813504008685845, 6.4811126326813415, 6.480875224751585, 6.4806438824709724, 6.480413791011362, 6.480187044424169, 6.47997004144332, 6.479750181822216, 6.47954192406991, 6.4793234169483185, 6.479120007332633, 6.478910420747364, 6.4787088273202675, 6.478505739394357, 6.4783079361214355, 6.478121064165059, 6.4779300268958595, 6.477744041120305, 6.477557792383082, 6.477371399016941, 6.477194577455521, 6.477017158971114, 6.476845732506583, 6.476676112588714, 6.476500449811711, 6.476332298096488, 6.4761648660196975, 6.4760017430081085, 6.475840998046539, 6.47568237518563, 6.475526812321999, 6.475375742596738, 6.475220886223457, 6.475066935314851, 6.474928386947688, 6.474782244247549, 6.474633948767886, 6.474495140068671, 6.474355872939615, 6.474208745886298, 6.474075258654707, 6.473939282052657, 6.473808323635774, 6.473674130790374, 6.473546573344399, 6.47342229152427, 6.473293596330811, 6.473166169489131, 6.473038501599255, 6.472923944978153, 6.4728069340481476, 6.472680843928281, 6.47256578680347, 6.47245126787354, 6.472341314834707, 6.472221932866994, 6.472115502637975, 6.472006665433154, 6.47190184014685, 6.471795773681472, 6.471690362867187, 6.471589434672804, 6.471485880367896, 6.471384983728914, 6.471284882110708, 6.471189780270352, 6.471089167629971, 6.470996462246951, 6.470901497146663, 6.470807499745312, 6.470714362228618, 6.470622908543138, 6.470533704056459, 6.470444056917639, 6.470354665728176, 6.470269024372101, 6.470183000845068, 6.470098354360637, 6.470011070370674, 6.469929213909542, 6.469855669666739, 6.469772848136285, 6.469691549153889, 6.469616712892757, 6.469539045410998, 6.469462344751639, 6.469386505730012, 6.469312660834369, 6.469236389679067, 6.46916752440088, 6.4690916757373245, 6.46901944805594, 6.468951035071822, 6.4688832558253235, 6.468810032395756, 6.468740906785516, 6.468675932463477, 6.468612277332475, 6.468544097507701, 6.468478458769181, 6.468413319657831, 6.468350158894763, 6.468287581030061, 6.468222620732644, 6.468163811108646, 6.468101090368102, 6.46804523906287, 6.467984145178514, 6.4679260920075805, 6.467867483987527, 6.467808639301973, 6.467754958307042, 6.467700122033849, 6.467647646279896, 6.467588621027329, 6.467534752453075, 6.467476204037666, 6.4674246889703415, 6.467372605905814, 6.4673249213134545, 6.467270459322369, 6.467221758821431, 6.467170971281388, 6.467123390120618, 6.467072746332954, 6.467024102807045, 6.466972164371434, 6.466929117546362, 6.466885405428269, 6.466832045246573, 6.466788557522437, 6.466744113494368]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(model_loss_record, title='deep model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8cTuQjQQOon",
    "outputId": "6bc5de07-4c5a-4e87-9ae3-d09f539c5f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to pred.csv\n"
     ]
    }
   ],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    print('Saving results to {}'.format(file))\n",
    "    with open(file, 'w', newline='') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['PerNo', 'PerStatus'])\n",
    "        for i, p in preds:\n",
    "            writer.writerow([i,p])\n",
    "model.eval() # set the model to evaluation mode\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs, idx in tt_set:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        preds.extend([[int(idx[i]), int(pred[i])]for i in range(pred.shape[0])])\n",
    "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2021Spring - HW1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
