{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tMj55YDKG6ch",
    "outputId": "fc40ecc9-4756-48b1-d5c6-c169a8b453b2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "\n",
    "seed = 6187\n",
    "random.seed = seed\n",
    "np.random.seed = seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_path = \"Data/Data_train/\"\n",
    "tt_path = \"Data/Data_test/\"\n",
    "tr_data = [[], [], []]\n",
    "tt_data = [[], [], []]\n",
    "\n",
    "for name in os.listdir(tr_path + \"Carambula\"):\n",
    "    image = Image.open(tr_path + \"Carambula/\" + name)\n",
    "    tr_data[0].append(np.array(image).reshape(32*32*2))\n",
    "for name in os.listdir(tr_path + \"Lychee\"):\n",
    "    image = Image.open(tr_path + \"Lychee/\" + name)\n",
    "    tr_data[1].append(np.array(image).reshape(32*32*2))\n",
    "for name in os.listdir(tr_path + \"Pear\"):\n",
    "    image = Image.open(tr_path + \"Pear/\" + name)\n",
    "    tr_data[2].append(np.array(image).reshape(32*32*2))\n",
    "    \n",
    "for name in os.listdir(tt_path + \"Carambula\"):\n",
    "    image = Image.open(tt_path + \"Carambula/\" + name)\n",
    "    tt_data[0].append(np.array(image).reshape(32*32*2))\n",
    "for name in os.listdir(tt_path + \"Lychee\"):\n",
    "    image = Image.open(tt_path + \"Lychee/\" + name)\n",
    "    tt_data[1].append(np.array(image).reshape(32*32*2))\n",
    "for name in os.listdir(tt_path + \"Pear\"):\n",
    "    image = Image.open(tt_path + \"Pear/\" + name)\n",
    "    tt_data[2].append(np.array(image).reshape(32*32*2))\n",
    "for i in range(3):\n",
    "    tr_data[i] = np.array(tr_data[i])\n",
    "    tt_data[i] = np.array(tt_data[i])\n",
    "\n",
    "for i in range(3):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(tr_data[i])\n",
    "    tr_data[i] = pca.transform(tr_data[i])\n",
    "    tt_data[i] = pca.transform(tt_data[i])\n",
    "    \n",
    "\n",
    "tr_in = []\n",
    "tt_in = []\n",
    "\n",
    "for i in range(3):\n",
    "    for q in range(len(tr_data[i])):\n",
    "        label = np.zeros((3))\n",
    "        label[i] = 1\n",
    "        tr_in.append([label, np.append(tr_data[i][q], np.array([1]))])\n",
    "    for q in range(len(tt_data[i])):\n",
    "        label = np.zeros((3))\n",
    "        label[i] = 1\n",
    "        tt_in.append([label, np.append(tt_data[i][q], np.array([1]))])\n",
    "        \n",
    "# random.shuffle(tr_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k-onQd4JNA5H"
   },
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For data preprocess\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "myseed = 42069  # set a random seed for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FWMT3uf1NGQp"
   },
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    ''' Get device (if GPU is available, use GPU) '''\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def plot_learning_curve(loss_record, title=''):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_steps = len(loss_record['train'])\n",
    "    x_1 = range(total_steps)\n",
    "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.ylim(0.0, 1.)\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0zlpIp9ANJRU"
   },
   "outputs": [],
   "source": [
    "tr_sz = 0\n",
    "dv_sz = 0\n",
    "class empDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 path,\n",
    "                 mode='train',\n",
    "                 target_only=False):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tr_in[index][1], tr_in[index][0]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the size of the dataset\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hlhLk5t6MBX3"
   },
   "outputs": [],
   "source": [
    "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
    "    ''' Generates a dataset, then is put into a dataloader. '''\n",
    "    dataset = empDataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size,\n",
    "        shuffle=(mode == 'train'), drop_last=False,\n",
    "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "49-uXYovOAI0"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight, gain=1.0)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    ''' A simple fully-connected deep neural network '''\n",
    "    def __init__(self, input_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.sigmoid(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,2),\n",
    "        )\n",
    "        self.net.apply(init_weights)\n",
    "        self.criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        ''' Calculate loss '''\n",
    "        return self.criterion(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lOqcmYzMO7jB"
   },
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "def train(tr_set, dv_set, model, config, device, tr_sz, dv_sz):\n",
    "    ''' DNN training '''\n",
    "\n",
    "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
    "        model.parameters(), **config['optim_hparas'])\n",
    "    global best_acc\n",
    "    epoch = 0\n",
    "    model_path = './model.ckpt'\n",
    "    loss_record = {\"train\":[], \"dev\":[]}\n",
    "    while epoch < n_epochs:\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train()                           # set model to training mode\n",
    "        for inputs, labels in tr_set:                     # iterate through the dataloader\n",
    "            inputs, labels = inputs.to(device), labels.to(device)   # move data to device (cpu/cuda)\n",
    "            outputs = model(inputs)                     # forward pass (compute output)\n",
    "#             print(outputs, labels, inputs)\n",
    "            batch_loss = model.cal_loss(outputs, labels)  # compute loss\n",
    "            _, train_pred = torch.max(outputs, 1) \n",
    "            batch_loss.backward()                 # compute gradient (backpropagation)\n",
    "            optimizer.step()                    # update model with optimizer\n",
    "            optimizer.zero_grad()               # set gradient to zero\n",
    "            train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "            train_loss += batch_loss.item()\n",
    "        loss_record[\"train\"].append(train_loss/len(tr_set))\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dv_set:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                batch_loss = model.cal_loss(outputs, labels) \n",
    "                _, val_pred = torch.max(outputs, 1) \n",
    "\n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item() # get the index of the class with the highest probability\n",
    "                val_loss += batch_loss.item()\n",
    "            loss_record[\"dev\"].append(val_loss/len(dv_set))\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, n_epochs, train_acc/tr_sz, train_loss/len(tr_set), val_acc/dv_sz, val_loss/len(dv_set)\n",
    "            ))\n",
    "            # if the model improves, save a checkpoint at this epoch\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(best_acc/dv_sz))\n",
    "\n",
    "        epoch += 1\n",
    "    print('Finished training after {} epochs'.format(epoch))\n",
    "    return val_loss, loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NPXpdumwPjE7"
   },
   "outputs": [],
   "source": [
    "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
    "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
    "target_only = True                 # TODO: Using 40 states & 2 tested_positive features\n",
    "\n",
    "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
    "config = {\n",
    "    'n_epochs': 300,                # maximum number of epochs\n",
    "    'batch_size': 256,               # mini-batch size for dataloader\n",
    "    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)\n",
    "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
    "        'lr': 0.0001,                 # learning rate of SGD\n",
    "#         'weight_decay':0.005,\n",
    "        'momentum': 0.9              # momentum for SGD\n",
    "    },\n",
    "    'early_stop': 1000,               # early stopping epochs (the number epochs since your model's last improvement)\n",
    "    'save_path': 'models/model.pth'  # your model will be saved here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNrYBMmePLKm",
    "outputId": "fcd4f175-4f7e-4306-f33c-5f8285f11dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of Dataset (4521 samples found, each dim = 81)\n",
      "Finished reading the dev set of Dataset (1935 samples found, each dim = 81)\n",
      "Finished reading the test set of Dataset (3739 samples found, each dim = 81)\n"
     ]
    }
   ],
   "source": [
    "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
    "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
    "# va_set = prep_dataloader(tr_path, 'valid', config['batch_size'], target_only=target_only)\n",
    "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FHylSirLP9oh"
   },
   "outputs": [],
   "source": [
    "model = Classifier(tr_set.dataset.dim).to(device)  # Construct model and move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GrEbUxazQAAZ",
    "outputId": "f4f3bd74-2d97-4275-b69f-6609976b91f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/300] Train Acc: 0.694979 Loss: 3899.000947 | Val Acc: 0.951938 loss: 0.503889\n",
      "saving model with acc 0.952\n",
      "[002/300] Train Acc: 0.953329 Loss: 4.883266 | Val Acc: 0.951938 loss: 0.471607\n",
      "[003/300] Train Acc: 0.953329 Loss: 4.151743 | Val Acc: 0.951938 loss: 0.347021\n",
      "[004/300] Train Acc: 0.953329 Loss: 3.852472 | Val Acc: 0.951938 loss: 0.254274\n",
      "[005/300] Train Acc: 0.953329 Loss: 3.093954 | Val Acc: 0.951938 loss: 0.226756\n",
      "[006/300] Train Acc: 0.953329 Loss: 2.190764 | Val Acc: 0.951938 loss: 0.224411\n",
      "[007/300] Train Acc: 0.953329 Loss: 3.690158 | Val Acc: 0.951938 loss: 0.233967\n",
      "[008/300] Train Acc: 0.953329 Loss: 5.262108 | Val Acc: 0.951938 loss: 0.224953\n",
      "[009/300] Train Acc: 0.953329 Loss: 3.881611 | Val Acc: 0.951938 loss: 0.224764\n",
      "[010/300] Train Acc: 0.953329 Loss: 2.577787 | Val Acc: 0.951938 loss: 0.224960\n",
      "[011/300] Train Acc: 0.953329 Loss: 2.309523 | Val Acc: 0.951938 loss: 0.225707\n",
      "[012/300] Train Acc: 0.953329 Loss: 1.606545 | Val Acc: 0.951938 loss: 0.224802\n",
      "[013/300] Train Acc: 0.953329 Loss: 0.989095 | Val Acc: 0.951938 loss: 0.227092\n",
      "[014/300] Train Acc: 0.953329 Loss: 0.826621 | Val Acc: 0.951938 loss: 0.225136\n",
      "[015/300] Train Acc: 0.953329 Loss: 0.725677 | Val Acc: 0.951938 loss: 0.226129\n",
      "[016/300] Train Acc: 0.953329 Loss: 0.669864 | Val Acc: 0.951938 loss: 0.225715\n",
      "[017/300] Train Acc: 0.953329 Loss: 0.604428 | Val Acc: 0.951938 loss: 0.225939\n",
      "[018/300] Train Acc: 0.953329 Loss: 0.552603 | Val Acc: 0.951938 loss: 0.225471\n",
      "[019/300] Train Acc: 0.953329 Loss: 0.499564 | Val Acc: 0.951938 loss: 0.226263\n",
      "[020/300] Train Acc: 0.953329 Loss: 0.450471 | Val Acc: 0.951938 loss: 0.225941\n",
      "[021/300] Train Acc: 0.953329 Loss: 0.407512 | Val Acc: 0.951938 loss: 0.225407\n",
      "[022/300] Train Acc: 0.953329 Loss: 0.363708 | Val Acc: 0.951938 loss: 0.226403\n",
      "[023/300] Train Acc: 0.953329 Loss: 0.341213 | Val Acc: 0.951938 loss: 0.225879\n",
      "[024/300] Train Acc: 0.953329 Loss: 0.282595 | Val Acc: 0.951938 loss: 0.226229\n",
      "[025/300] Train Acc: 0.953329 Loss: 0.237232 | Val Acc: 0.951938 loss: 0.226066\n",
      "[026/300] Train Acc: 0.953329 Loss: 0.220897 | Val Acc: 0.951938 loss: 0.225866\n",
      "[027/300] Train Acc: 0.953329 Loss: 0.195521 | Val Acc: 0.951938 loss: 0.227182\n",
      "[028/300] Train Acc: 0.953329 Loss: 0.192451 | Val Acc: 0.951938 loss: 0.226027\n",
      "[029/300] Train Acc: 0.953329 Loss: 0.191579 | Val Acc: 0.951938 loss: 0.226291\n",
      "[030/300] Train Acc: 0.953329 Loss: 0.190789 | Val Acc: 0.951938 loss: 0.225923\n",
      "[031/300] Train Acc: 0.953329 Loss: 0.190520 | Val Acc: 0.951938 loss: 0.226674\n",
      "[032/300] Train Acc: 0.953329 Loss: 0.189155 | Val Acc: 0.951938 loss: 0.225742\n",
      "[033/300] Train Acc: 0.953329 Loss: 0.190866 | Val Acc: 0.951938 loss: 0.226975\n",
      "[034/300] Train Acc: 0.953329 Loss: 0.188250 | Val Acc: 0.951938 loss: 0.226533\n",
      "[035/300] Train Acc: 0.953329 Loss: 0.188015 | Val Acc: 0.951938 loss: 0.227689\n",
      "[036/300] Train Acc: 0.953329 Loss: 0.189912 | Val Acc: 0.951938 loss: 0.227195\n",
      "[037/300] Train Acc: 0.953329 Loss: 0.186893 | Val Acc: 0.951938 loss: 0.227021\n",
      "[038/300] Train Acc: 0.953329 Loss: 0.189669 | Val Acc: 0.951938 loss: 0.227428\n",
      "[039/300] Train Acc: 0.953329 Loss: 0.188981 | Val Acc: 0.951938 loss: 0.227221\n",
      "[040/300] Train Acc: 0.953329 Loss: 0.187551 | Val Acc: 0.951938 loss: 0.226891\n",
      "[041/300] Train Acc: 0.953329 Loss: 0.188593 | Val Acc: 0.951938 loss: 0.226691\n",
      "[042/300] Train Acc: 0.953329 Loss: 0.189366 | Val Acc: 0.951938 loss: 0.227217\n",
      "[043/300] Train Acc: 0.953329 Loss: 0.189582 | Val Acc: 0.951938 loss: 0.227197\n",
      "[044/300] Train Acc: 0.953329 Loss: 0.189099 | Val Acc: 0.951938 loss: 0.227150\n",
      "[045/300] Train Acc: 0.953329 Loss: 0.187607 | Val Acc: 0.951938 loss: 0.227242\n",
      "[046/300] Train Acc: 0.953329 Loss: 0.188660 | Val Acc: 0.951938 loss: 0.226760\n",
      "[047/300] Train Acc: 0.953329 Loss: 0.189027 | Val Acc: 0.951938 loss: 0.227639\n",
      "[048/300] Train Acc: 0.953329 Loss: 0.188226 | Val Acc: 0.951938 loss: 0.226636\n",
      "[049/300] Train Acc: 0.953329 Loss: 0.187895 | Val Acc: 0.951938 loss: 0.226843\n",
      "[050/300] Train Acc: 0.953329 Loss: 0.188450 | Val Acc: 0.951938 loss: 0.227671\n",
      "[051/300] Train Acc: 0.953329 Loss: 0.189961 | Val Acc: 0.951938 loss: 0.226857\n",
      "[052/300] Train Acc: 0.953329 Loss: 0.187498 | Val Acc: 0.951938 loss: 0.227414\n",
      "[053/300] Train Acc: 0.953329 Loss: 0.188879 | Val Acc: 0.951938 loss: 0.227131\n",
      "[054/300] Train Acc: 0.953329 Loss: 0.189223 | Val Acc: 0.951938 loss: 0.227003\n",
      "[055/300] Train Acc: 0.953329 Loss: 0.189066 | Val Acc: 0.951938 loss: 0.227213\n",
      "[056/300] Train Acc: 0.953329 Loss: 0.187325 | Val Acc: 0.951938 loss: 0.226561\n",
      "[057/300] Train Acc: 0.953329 Loss: 0.186780 | Val Acc: 0.951938 loss: 0.227762\n",
      "[058/300] Train Acc: 0.953329 Loss: 0.189954 | Val Acc: 0.951938 loss: 0.227417\n",
      "[059/300] Train Acc: 0.953329 Loss: 0.188551 | Val Acc: 0.951938 loss: 0.227301\n",
      "[060/300] Train Acc: 0.953329 Loss: 0.189271 | Val Acc: 0.951938 loss: 0.227660\n",
      "[061/300] Train Acc: 0.953329 Loss: 0.187549 | Val Acc: 0.951938 loss: 0.226654\n",
      "[062/300] Train Acc: 0.953329 Loss: 0.188268 | Val Acc: 0.951938 loss: 0.227395\n",
      "[063/300] Train Acc: 0.953329 Loss: 0.187016 | Val Acc: 0.951938 loss: 0.227297\n",
      "[064/300] Train Acc: 0.953329 Loss: 0.187606 | Val Acc: 0.951938 loss: 0.227715\n",
      "[065/300] Train Acc: 0.953329 Loss: 0.187329 | Val Acc: 0.951938 loss: 0.227370\n",
      "[066/300] Train Acc: 0.953329 Loss: 0.189270 | Val Acc: 0.951938 loss: 0.228395\n",
      "[067/300] Train Acc: 0.953329 Loss: 0.188671 | Val Acc: 0.951938 loss: 0.226831\n",
      "[068/300] Train Acc: 0.953329 Loss: 0.189182 | Val Acc: 0.951938 loss: 0.227479\n",
      "[069/300] Train Acc: 0.953329 Loss: 0.188181 | Val Acc: 0.951938 loss: 0.226970\n",
      "[070/300] Train Acc: 0.953329 Loss: 0.188228 | Val Acc: 0.951938 loss: 0.226929\n",
      "[071/300] Train Acc: 0.953329 Loss: 0.188145 | Val Acc: 0.951938 loss: 0.226811\n",
      "[072/300] Train Acc: 0.953329 Loss: 0.190458 | Val Acc: 0.951938 loss: 0.227673\n",
      "[073/300] Train Acc: 0.953329 Loss: 0.188686 | Val Acc: 0.951938 loss: 0.227444\n",
      "[074/300] Train Acc: 0.953329 Loss: 0.187489 | Val Acc: 0.951938 loss: 0.227051\n",
      "[075/300] Train Acc: 0.953329 Loss: 0.188546 | Val Acc: 0.951938 loss: 0.227885\n",
      "[076/300] Train Acc: 0.953329 Loss: 0.188041 | Val Acc: 0.951938 loss: 0.226874\n",
      "[077/300] Train Acc: 0.953329 Loss: 0.188930 | Val Acc: 0.951938 loss: 0.227411\n",
      "[078/300] Train Acc: 0.953329 Loss: 0.189608 | Val Acc: 0.951938 loss: 0.227178\n",
      "[079/300] Train Acc: 0.953329 Loss: 0.188326 | Val Acc: 0.951938 loss: 0.227350\n",
      "[080/300] Train Acc: 0.953329 Loss: 0.189413 | Val Acc: 0.951938 loss: 0.227422\n",
      "[081/300] Train Acc: 0.953329 Loss: 0.187927 | Val Acc: 0.951938 loss: 0.227200\n",
      "[082/300] Train Acc: 0.953329 Loss: 0.187839 | Val Acc: 0.951938 loss: 0.227599\n",
      "[083/300] Train Acc: 0.953329 Loss: 0.188984 | Val Acc: 0.951938 loss: 0.227816\n",
      "[084/300] Train Acc: 0.953329 Loss: 0.188270 | Val Acc: 0.951938 loss: 0.227348\n",
      "[085/300] Train Acc: 0.953329 Loss: 0.188188 | Val Acc: 0.951938 loss: 0.227336\n",
      "[086/300] Train Acc: 0.953329 Loss: 0.189971 | Val Acc: 0.951938 loss: 0.228112\n",
      "[087/300] Train Acc: 0.953329 Loss: 0.187206 | Val Acc: 0.951938 loss: 0.226817\n",
      "[088/300] Train Acc: 0.953329 Loss: 0.189572 | Val Acc: 0.951938 loss: 0.227248\n",
      "[089/300] Train Acc: 0.953329 Loss: 0.187563 | Val Acc: 0.951938 loss: 0.227292\n",
      "[090/300] Train Acc: 0.953329 Loss: 0.188841 | Val Acc: 0.951938 loss: 0.227518\n",
      "[091/300] Train Acc: 0.953329 Loss: 0.188273 | Val Acc: 0.951938 loss: 0.227928\n",
      "[092/300] Train Acc: 0.953329 Loss: 0.187877 | Val Acc: 0.951938 loss: 0.227138\n",
      "[093/300] Train Acc: 0.953329 Loss: 0.188879 | Val Acc: 0.951938 loss: 0.227679\n",
      "[094/300] Train Acc: 0.953329 Loss: 0.187701 | Val Acc: 0.951938 loss: 0.226382\n",
      "[095/300] Train Acc: 0.953329 Loss: 0.189920 | Val Acc: 0.951938 loss: 0.228856\n",
      "[096/300] Train Acc: 0.953329 Loss: 0.189402 | Val Acc: 0.951938 loss: 0.227115\n",
      "[097/300] Train Acc: 0.953329 Loss: 0.190333 | Val Acc: 0.951938 loss: 0.227327\n",
      "[098/300] Train Acc: 0.953329 Loss: 0.188685 | Val Acc: 0.951938 loss: 0.226894\n",
      "[099/300] Train Acc: 0.953329 Loss: 0.187274 | Val Acc: 0.951938 loss: 0.227511\n",
      "[100/300] Train Acc: 0.953329 Loss: 0.189878 | Val Acc: 0.951938 loss: 0.227591\n",
      "[101/300] Train Acc: 0.953329 Loss: 0.187918 | Val Acc: 0.951938 loss: 0.226589\n",
      "[102/300] Train Acc: 0.953329 Loss: 0.187916 | Val Acc: 0.951938 loss: 0.227225\n",
      "[103/300] Train Acc: 0.953329 Loss: 0.187447 | Val Acc: 0.951938 loss: 0.227956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104/300] Train Acc: 0.953329 Loss: 0.188189 | Val Acc: 0.951938 loss: 0.227969\n",
      "[105/300] Train Acc: 0.953329 Loss: 0.187624 | Val Acc: 0.951938 loss: 0.226690\n",
      "[106/300] Train Acc: 0.953329 Loss: 0.189535 | Val Acc: 0.951938 loss: 0.228092\n",
      "[107/300] Train Acc: 0.953329 Loss: 0.188904 | Val Acc: 0.951938 loss: 0.226648\n",
      "[108/300] Train Acc: 0.953329 Loss: 0.188178 | Val Acc: 0.951938 loss: 0.228611\n",
      "[109/300] Train Acc: 0.953329 Loss: 0.187726 | Val Acc: 0.951938 loss: 0.227159\n",
      "[110/300] Train Acc: 0.953329 Loss: 0.188555 | Val Acc: 0.951938 loss: 0.227267\n",
      "[111/300] Train Acc: 0.953329 Loss: 0.189287 | Val Acc: 0.951938 loss: 0.227861\n",
      "[112/300] Train Acc: 0.953329 Loss: 0.188023 | Val Acc: 0.951938 loss: 0.226560\n",
      "[113/300] Train Acc: 0.953329 Loss: 0.189462 | Val Acc: 0.951938 loss: 0.227776\n",
      "[114/300] Train Acc: 0.953329 Loss: 0.187599 | Val Acc: 0.951938 loss: 0.227158\n",
      "[115/300] Train Acc: 0.953329 Loss: 0.187919 | Val Acc: 0.951938 loss: 0.227896\n",
      "[116/300] Train Acc: 0.953329 Loss: 0.187482 | Val Acc: 0.951938 loss: 0.226970\n",
      "[117/300] Train Acc: 0.953329 Loss: 0.187574 | Val Acc: 0.951938 loss: 0.227899\n",
      "[118/300] Train Acc: 0.953329 Loss: 0.188887 | Val Acc: 0.951938 loss: 0.227829\n",
      "[119/300] Train Acc: 0.953329 Loss: 0.190718 | Val Acc: 0.951938 loss: 0.227045\n",
      "[120/300] Train Acc: 0.953329 Loss: 0.188841 | Val Acc: 0.951938 loss: 0.227675\n",
      "[121/300] Train Acc: 0.953329 Loss: 0.187300 | Val Acc: 0.951938 loss: 0.227571\n",
      "[122/300] Train Acc: 0.953329 Loss: 0.190065 | Val Acc: 0.951938 loss: 0.227921\n",
      "[123/300] Train Acc: 0.953329 Loss: 0.188683 | Val Acc: 0.951938 loss: 0.226665\n",
      "[124/300] Train Acc: 0.953329 Loss: 0.189536 | Val Acc: 0.951938 loss: 0.227763\n",
      "[125/300] Train Acc: 0.953329 Loss: 0.188639 | Val Acc: 0.951938 loss: 0.227636\n",
      "[126/300] Train Acc: 0.953329 Loss: 0.188209 | Val Acc: 0.951938 loss: 0.228166\n",
      "[127/300] Train Acc: 0.953329 Loss: 0.188945 | Val Acc: 0.951938 loss: 0.227306\n",
      "[128/300] Train Acc: 0.953329 Loss: 0.187162 | Val Acc: 0.951938 loss: 0.227841\n",
      "[129/300] Train Acc: 0.953329 Loss: 0.188158 | Val Acc: 0.951938 loss: 0.227709\n",
      "[130/300] Train Acc: 0.953329 Loss: 0.190635 | Val Acc: 0.951938 loss: 0.227892\n",
      "[131/300] Train Acc: 0.953329 Loss: 0.187542 | Val Acc: 0.951938 loss: 0.226875\n",
      "[132/300] Train Acc: 0.953329 Loss: 0.187457 | Val Acc: 0.951938 loss: 0.227545\n",
      "[133/300] Train Acc: 0.953329 Loss: 0.190576 | Val Acc: 0.951938 loss: 0.227980\n",
      "[134/300] Train Acc: 0.953329 Loss: 0.188516 | Val Acc: 0.951938 loss: 0.227147\n",
      "[135/300] Train Acc: 0.953329 Loss: 0.188456 | Val Acc: 0.951938 loss: 0.227452\n",
      "[136/300] Train Acc: 0.953329 Loss: 0.187115 | Val Acc: 0.951938 loss: 0.228576\n",
      "[137/300] Train Acc: 0.953329 Loss: 0.187881 | Val Acc: 0.951938 loss: 0.227425\n",
      "[138/300] Train Acc: 0.953329 Loss: 0.189155 | Val Acc: 0.951938 loss: 0.227386\n",
      "[139/300] Train Acc: 0.953329 Loss: 0.186895 | Val Acc: 0.951938 loss: 0.226970\n",
      "[140/300] Train Acc: 0.953329 Loss: 0.187565 | Val Acc: 0.951938 loss: 0.227704\n",
      "[141/300] Train Acc: 0.953329 Loss: 0.187122 | Val Acc: 0.951938 loss: 0.228075\n",
      "[142/300] Train Acc: 0.953329 Loss: 0.187600 | Val Acc: 0.951938 loss: 0.227271\n",
      "[143/300] Train Acc: 0.953329 Loss: 0.188747 | Val Acc: 0.951938 loss: 0.227848\n",
      "[144/300] Train Acc: 0.953329 Loss: 0.187555 | Val Acc: 0.951938 loss: 0.227334\n",
      "[145/300] Train Acc: 0.953329 Loss: 0.187871 | Val Acc: 0.951938 loss: 0.227441\n",
      "[146/300] Train Acc: 0.953329 Loss: 0.189364 | Val Acc: 0.951938 loss: 0.227600\n",
      "[147/300] Train Acc: 0.953329 Loss: 0.188142 | Val Acc: 0.951938 loss: 0.227371\n",
      "[148/300] Train Acc: 0.953329 Loss: 0.187817 | Val Acc: 0.951938 loss: 0.227880\n",
      "[149/300] Train Acc: 0.953329 Loss: 0.189608 | Val Acc: 0.951938 loss: 0.227446\n",
      "[150/300] Train Acc: 0.953329 Loss: 0.190149 | Val Acc: 0.951938 loss: 0.227534\n",
      "[151/300] Train Acc: 0.953329 Loss: 0.188930 | Val Acc: 0.951938 loss: 0.227517\n",
      "[152/300] Train Acc: 0.953329 Loss: 0.187517 | Val Acc: 0.951938 loss: 0.227406\n",
      "[153/300] Train Acc: 0.953329 Loss: 0.188215 | Val Acc: 0.951938 loss: 0.227577\n",
      "[154/300] Train Acc: 0.953329 Loss: 0.188161 | Val Acc: 0.951938 loss: 0.227237\n",
      "[155/300] Train Acc: 0.953329 Loss: 0.189687 | Val Acc: 0.951938 loss: 0.227708\n",
      "[156/300] Train Acc: 0.953329 Loss: 0.188605 | Val Acc: 0.951938 loss: 0.228149\n",
      "[157/300] Train Acc: 0.953329 Loss: 0.189123 | Val Acc: 0.951938 loss: 0.226749\n",
      "[158/300] Train Acc: 0.953329 Loss: 0.190331 | Val Acc: 0.951938 loss: 0.227853\n",
      "[159/300] Train Acc: 0.953329 Loss: 0.188467 | Val Acc: 0.951938 loss: 0.227023\n",
      "[160/300] Train Acc: 0.953329 Loss: 0.187971 | Val Acc: 0.951938 loss: 0.227706\n",
      "[161/300] Train Acc: 0.953329 Loss: 0.188221 | Val Acc: 0.951938 loss: 0.227224\n",
      "[162/300] Train Acc: 0.953329 Loss: 0.187571 | Val Acc: 0.951938 loss: 0.227430\n",
      "[163/300] Train Acc: 0.953329 Loss: 0.187470 | Val Acc: 0.951938 loss: 0.227337\n",
      "[164/300] Train Acc: 0.953329 Loss: 0.187561 | Val Acc: 0.951938 loss: 0.227494\n",
      "[165/300] Train Acc: 0.953329 Loss: 0.188185 | Val Acc: 0.951938 loss: 0.227232\n",
      "[166/300] Train Acc: 0.953329 Loss: 0.190326 | Val Acc: 0.951938 loss: 0.228317\n",
      "[167/300] Train Acc: 0.953329 Loss: 0.188218 | Val Acc: 0.951938 loss: 0.226758\n",
      "[168/300] Train Acc: 0.953329 Loss: 0.187444 | Val Acc: 0.951938 loss: 0.228251\n",
      "[169/300] Train Acc: 0.953329 Loss: 0.187523 | Val Acc: 0.951938 loss: 0.227282\n",
      "[170/300] Train Acc: 0.953329 Loss: 0.188098 | Val Acc: 0.951938 loss: 0.228013\n",
      "[171/300] Train Acc: 0.953329 Loss: 0.188881 | Val Acc: 0.951938 loss: 0.227986\n",
      "[172/300] Train Acc: 0.953329 Loss: 0.189898 | Val Acc: 0.951938 loss: 0.226830\n",
      "[173/300] Train Acc: 0.953329 Loss: 0.188187 | Val Acc: 0.951938 loss: 0.227638\n",
      "[174/300] Train Acc: 0.953329 Loss: 0.189930 | Val Acc: 0.951938 loss: 0.227646\n",
      "[175/300] Train Acc: 0.953329 Loss: 0.189338 | Val Acc: 0.951938 loss: 0.226980\n",
      "[176/300] Train Acc: 0.953329 Loss: 0.189534 | Val Acc: 0.951938 loss: 0.228337\n",
      "[177/300] Train Acc: 0.953329 Loss: 0.188418 | Val Acc: 0.951938 loss: 0.227492\n",
      "[178/300] Train Acc: 0.953329 Loss: 0.190250 | Val Acc: 0.951938 loss: 0.227326\n",
      "[179/300] Train Acc: 0.953329 Loss: 0.187906 | Val Acc: 0.951938 loss: 0.227297\n",
      "[180/300] Train Acc: 0.953329 Loss: 0.187663 | Val Acc: 0.951938 loss: 0.227976\n",
      "[181/300] Train Acc: 0.953329 Loss: 0.189462 | Val Acc: 0.951938 loss: 0.228872\n",
      "[182/300] Train Acc: 0.953329 Loss: 0.188744 | Val Acc: 0.951938 loss: 0.226817\n",
      "[183/300] Train Acc: 0.953329 Loss: 0.189307 | Val Acc: 0.951938 loss: 0.227675\n",
      "[184/300] Train Acc: 0.953329 Loss: 0.189025 | Val Acc: 0.951938 loss: 0.227853\n",
      "[185/300] Train Acc: 0.953329 Loss: 0.188784 | Val Acc: 0.951938 loss: 0.228016\n",
      "[186/300] Train Acc: 0.953329 Loss: 0.188542 | Val Acc: 0.951938 loss: 0.227410\n",
      "[187/300] Train Acc: 0.953329 Loss: 0.189360 | Val Acc: 0.951938 loss: 0.228757\n",
      "[188/300] Train Acc: 0.953329 Loss: 0.188583 | Val Acc: 0.951938 loss: 0.226814\n",
      "[189/300] Train Acc: 0.953329 Loss: 0.189641 | Val Acc: 0.951938 loss: 0.227796\n",
      "[190/300] Train Acc: 0.953329 Loss: 0.189169 | Val Acc: 0.951938 loss: 0.227247\n",
      "[191/300] Train Acc: 0.953329 Loss: 0.187476 | Val Acc: 0.951938 loss: 0.227946\n",
      "[192/300] Train Acc: 0.953329 Loss: 0.189308 | Val Acc: 0.951938 loss: 0.228392\n",
      "[193/300] Train Acc: 0.953329 Loss: 0.188225 | Val Acc: 0.951938 loss: 0.227313\n",
      "[194/300] Train Acc: 0.953329 Loss: 0.188665 | Val Acc: 0.951938 loss: 0.228190\n",
      "[195/300] Train Acc: 0.953329 Loss: 0.187833 | Val Acc: 0.951938 loss: 0.227121\n",
      "[196/300] Train Acc: 0.953329 Loss: 0.188274 | Val Acc: 0.951938 loss: 0.228121\n",
      "[197/300] Train Acc: 0.953329 Loss: 0.187942 | Val Acc: 0.951938 loss: 0.227641\n",
      "[198/300] Train Acc: 0.953329 Loss: 0.188824 | Val Acc: 0.951938 loss: 0.228163\n",
      "[199/300] Train Acc: 0.953329 Loss: 0.189330 | Val Acc: 0.951938 loss: 0.227856\n",
      "[200/300] Train Acc: 0.953329 Loss: 0.186821 | Val Acc: 0.951938 loss: 0.227236\n",
      "[201/300] Train Acc: 0.953329 Loss: 0.189228 | Val Acc: 0.951938 loss: 0.227957\n",
      "[202/300] Train Acc: 0.953329 Loss: 0.187657 | Val Acc: 0.951938 loss: 0.227479\n",
      "[203/300] Train Acc: 0.953329 Loss: 0.188327 | Val Acc: 0.951938 loss: 0.228713\n",
      "[204/300] Train Acc: 0.953329 Loss: 0.188964 | Val Acc: 0.951938 loss: 0.227666\n",
      "[205/300] Train Acc: 0.953329 Loss: 0.186752 | Val Acc: 0.951938 loss: 0.227131\n",
      "[206/300] Train Acc: 0.953329 Loss: 0.189170 | Val Acc: 0.951938 loss: 0.227810\n",
      "[207/300] Train Acc: 0.953329 Loss: 0.190223 | Val Acc: 0.951938 loss: 0.228205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208/300] Train Acc: 0.953329 Loss: 0.189744 | Val Acc: 0.951938 loss: 0.226990\n",
      "[209/300] Train Acc: 0.953329 Loss: 0.187944 | Val Acc: 0.951938 loss: 0.227557\n",
      "[210/300] Train Acc: 0.953329 Loss: 0.187243 | Val Acc: 0.951938 loss: 0.226964\n",
      "[211/300] Train Acc: 0.953329 Loss: 0.187338 | Val Acc: 0.951938 loss: 0.228891\n",
      "[212/300] Train Acc: 0.953329 Loss: 0.188987 | Val Acc: 0.951938 loss: 0.227943\n",
      "[213/300] Train Acc: 0.953329 Loss: 0.188658 | Val Acc: 0.951938 loss: 0.227727\n",
      "[214/300] Train Acc: 0.953329 Loss: 0.187517 | Val Acc: 0.951938 loss: 0.227369\n",
      "[215/300] Train Acc: 0.953329 Loss: 0.188491 | Val Acc: 0.951938 loss: 0.227979\n",
      "[216/300] Train Acc: 0.953329 Loss: 0.190176 | Val Acc: 0.951938 loss: 0.228686\n",
      "[217/300] Train Acc: 0.953329 Loss: 0.188333 | Val Acc: 0.951938 loss: 0.227260\n",
      "[218/300] Train Acc: 0.953329 Loss: 0.187502 | Val Acc: 0.951938 loss: 0.227974\n",
      "[219/300] Train Acc: 0.953329 Loss: 0.188457 | Val Acc: 0.951938 loss: 0.227769\n",
      "[220/300] Train Acc: 0.953329 Loss: 0.188011 | Val Acc: 0.951938 loss: 0.227511\n",
      "[221/300] Train Acc: 0.953329 Loss: 0.187613 | Val Acc: 0.951938 loss: 0.227944\n",
      "[222/300] Train Acc: 0.953329 Loss: 0.187940 | Val Acc: 0.951938 loss: 0.227468\n",
      "[223/300] Train Acc: 0.953329 Loss: 0.189942 | Val Acc: 0.951938 loss: 0.228352\n",
      "[224/300] Train Acc: 0.953329 Loss: 0.187247 | Val Acc: 0.951938 loss: 0.227625\n",
      "[225/300] Train Acc: 0.953329 Loss: 0.188354 | Val Acc: 0.951938 loss: 0.227388\n",
      "[226/300] Train Acc: 0.953329 Loss: 0.189521 | Val Acc: 0.951938 loss: 0.228276\n",
      "[227/300] Train Acc: 0.953329 Loss: 0.187783 | Val Acc: 0.951938 loss: 0.227571\n",
      "[228/300] Train Acc: 0.953329 Loss: 0.189498 | Val Acc: 0.951938 loss: 0.227952\n",
      "[229/300] Train Acc: 0.953329 Loss: 0.188516 | Val Acc: 0.951938 loss: 0.226808\n",
      "[230/300] Train Acc: 0.953329 Loss: 0.187647 | Val Acc: 0.951938 loss: 0.228771\n",
      "[231/300] Train Acc: 0.953329 Loss: 0.188296 | Val Acc: 0.951938 loss: 0.227654\n",
      "[232/300] Train Acc: 0.953329 Loss: 0.188269 | Val Acc: 0.951938 loss: 0.227562\n",
      "[233/300] Train Acc: 0.953329 Loss: 0.188517 | Val Acc: 0.951938 loss: 0.227719\n",
      "[234/300] Train Acc: 0.953329 Loss: 0.187032 | Val Acc: 0.951938 loss: 0.227461\n",
      "[235/300] Train Acc: 0.953329 Loss: 0.189364 | Val Acc: 0.951938 loss: 0.228030\n",
      "[236/300] Train Acc: 0.953329 Loss: 0.190063 | Val Acc: 0.951938 loss: 0.227241\n",
      "[237/300] Train Acc: 0.953329 Loss: 0.189007 | Val Acc: 0.951938 loss: 0.227507\n",
      "[238/300] Train Acc: 0.953329 Loss: 0.186773 | Val Acc: 0.951938 loss: 0.227634\n",
      "[239/300] Train Acc: 0.953329 Loss: 0.188510 | Val Acc: 0.951938 loss: 0.228025\n",
      "[240/300] Train Acc: 0.953329 Loss: 0.186990 | Val Acc: 0.951938 loss: 0.228265\n",
      "[241/300] Train Acc: 0.953329 Loss: 0.189286 | Val Acc: 0.951938 loss: 0.226935\n",
      "[242/300] Train Acc: 0.953329 Loss: 0.186101 | Val Acc: 0.951938 loss: 0.227334\n",
      "[243/300] Train Acc: 0.953329 Loss: 0.187728 | Val Acc: 0.951938 loss: 0.228262\n",
      "[244/300] Train Acc: 0.953329 Loss: 0.188502 | Val Acc: 0.951938 loss: 0.228210\n",
      "[245/300] Train Acc: 0.953329 Loss: 0.187738 | Val Acc: 0.951938 loss: 0.227396\n",
      "[246/300] Train Acc: 0.953329 Loss: 0.187985 | Val Acc: 0.951938 loss: 0.228225\n",
      "[247/300] Train Acc: 0.953329 Loss: 0.188183 | Val Acc: 0.951938 loss: 0.227543\n",
      "[248/300] Train Acc: 0.953329 Loss: 0.188657 | Val Acc: 0.951938 loss: 0.227485\n",
      "[249/300] Train Acc: 0.953329 Loss: 0.189260 | Val Acc: 0.951938 loss: 0.227894\n",
      "[250/300] Train Acc: 0.953329 Loss: 0.189371 | Val Acc: 0.951938 loss: 0.227982\n",
      "[251/300] Train Acc: 0.953329 Loss: 0.189218 | Val Acc: 0.951938 loss: 0.227870\n",
      "[252/300] Train Acc: 0.953329 Loss: 0.187759 | Val Acc: 0.951938 loss: 0.227366\n",
      "[253/300] Train Acc: 0.953329 Loss: 0.188217 | Val Acc: 0.951938 loss: 0.227722\n",
      "[254/300] Train Acc: 0.953329 Loss: 0.188468 | Val Acc: 0.951938 loss: 0.227602\n",
      "[255/300] Train Acc: 0.953329 Loss: 0.187087 | Val Acc: 0.951938 loss: 0.227884\n",
      "[256/300] Train Acc: 0.953329 Loss: 0.188441 | Val Acc: 0.951938 loss: 0.228164\n",
      "[257/300] Train Acc: 0.953329 Loss: 0.188737 | Val Acc: 0.951938 loss: 0.227259\n",
      "[258/300] Train Acc: 0.953329 Loss: 0.189262 | Val Acc: 0.951938 loss: 0.227828\n",
      "[259/300] Train Acc: 0.953329 Loss: 0.187510 | Val Acc: 0.951938 loss: 0.227423\n",
      "[260/300] Train Acc: 0.953329 Loss: 0.188171 | Val Acc: 0.951938 loss: 0.227892\n",
      "[261/300] Train Acc: 0.953329 Loss: 0.189159 | Val Acc: 0.951938 loss: 0.227635\n",
      "[262/300] Train Acc: 0.953329 Loss: 0.189102 | Val Acc: 0.951938 loss: 0.227759\n",
      "[263/300] Train Acc: 0.953329 Loss: 0.188425 | Val Acc: 0.951938 loss: 0.227474\n",
      "[264/300] Train Acc: 0.953329 Loss: 0.189209 | Val Acc: 0.951938 loss: 0.227876\n",
      "[265/300] Train Acc: 0.953329 Loss: 0.188135 | Val Acc: 0.951938 loss: 0.227419\n",
      "[266/300] Train Acc: 0.953329 Loss: 0.188062 | Val Acc: 0.951938 loss: 0.227385\n",
      "[267/300] Train Acc: 0.953329 Loss: 0.187940 | Val Acc: 0.951938 loss: 0.227530\n",
      "[268/300] Train Acc: 0.953329 Loss: 0.188519 | Val Acc: 0.951938 loss: 0.228387\n",
      "[269/300] Train Acc: 0.953329 Loss: 0.188569 | Val Acc: 0.951938 loss: 0.227405\n",
      "[270/300] Train Acc: 0.953329 Loss: 0.188790 | Val Acc: 0.951938 loss: 0.228255\n",
      "[271/300] Train Acc: 0.953329 Loss: 0.188281 | Val Acc: 0.951938 loss: 0.227231\n",
      "[272/300] Train Acc: 0.953329 Loss: 0.188978 | Val Acc: 0.951938 loss: 0.227890\n",
      "[273/300] Train Acc: 0.953329 Loss: 0.188514 | Val Acc: 0.951938 loss: 0.228471\n",
      "[274/300] Train Acc: 0.953329 Loss: 0.188126 | Val Acc: 0.951938 loss: 0.227094\n",
      "[275/300] Train Acc: 0.953329 Loss: 0.187741 | Val Acc: 0.951938 loss: 0.228226\n",
      "[276/300] Train Acc: 0.953329 Loss: 0.188797 | Val Acc: 0.951938 loss: 0.227663\n",
      "[277/300] Train Acc: 0.953329 Loss: 0.187189 | Val Acc: 0.951938 loss: 0.227064\n",
      "[278/300] Train Acc: 0.953329 Loss: 0.190211 | Val Acc: 0.951938 loss: 0.228202\n",
      "[279/300] Train Acc: 0.953329 Loss: 0.187771 | Val Acc: 0.951938 loss: 0.227100\n",
      "[280/300] Train Acc: 0.953329 Loss: 0.189264 | Val Acc: 0.951938 loss: 0.227781\n",
      "[281/300] Train Acc: 0.953329 Loss: 0.188478 | Val Acc: 0.951938 loss: 0.227583\n",
      "[282/300] Train Acc: 0.953329 Loss: 0.189152 | Val Acc: 0.951938 loss: 0.228137\n",
      "[283/300] Train Acc: 0.953329 Loss: 0.189054 | Val Acc: 0.951938 loss: 0.227162\n",
      "[284/300] Train Acc: 0.953329 Loss: 0.189133 | Val Acc: 0.951938 loss: 0.228499\n",
      "[285/300] Train Acc: 0.953329 Loss: 0.187476 | Val Acc: 0.951938 loss: 0.227488\n",
      "[286/300] Train Acc: 0.953329 Loss: 0.187612 | Val Acc: 0.951938 loss: 0.227505\n",
      "[287/300] Train Acc: 0.953329 Loss: 0.188135 | Val Acc: 0.951938 loss: 0.227477\n",
      "[288/300] Train Acc: 0.953329 Loss: 0.189194 | Val Acc: 0.951938 loss: 0.228761\n",
      "[289/300] Train Acc: 0.953329 Loss: 0.188796 | Val Acc: 0.951938 loss: 0.227205\n",
      "[290/300] Train Acc: 0.953329 Loss: 0.188492 | Val Acc: 0.951938 loss: 0.227261\n",
      "[291/300] Train Acc: 0.953329 Loss: 0.187481 | Val Acc: 0.951938 loss: 0.227837\n",
      "[292/300] Train Acc: 0.953329 Loss: 0.188131 | Val Acc: 0.951938 loss: 0.227911\n",
      "[293/300] Train Acc: 0.953329 Loss: 0.189894 | Val Acc: 0.951938 loss: 0.228271\n",
      "[294/300] Train Acc: 0.953329 Loss: 0.188126 | Val Acc: 0.951938 loss: 0.226979\n",
      "[295/300] Train Acc: 0.953329 Loss: 0.187732 | Val Acc: 0.951938 loss: 0.227683\n",
      "[296/300] Train Acc: 0.953329 Loss: 0.187577 | Val Acc: 0.951938 loss: 0.227750\n",
      "[297/300] Train Acc: 0.953329 Loss: 0.189462 | Val Acc: 0.951938 loss: 0.228275\n",
      "[298/300] Train Acc: 0.953329 Loss: 0.189431 | Val Acc: 0.951938 loss: 0.227797\n",
      "[299/300] Train Acc: 0.953329 Loss: 0.186500 | Val Acc: 0.951938 loss: 0.227960\n",
      "[300/300] Train Acc: 0.953329 Loss: 0.189577 | Val Acc: 0.951938 loss: 0.227697\n",
      "Finished training after 300 epochs\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device, tr_sz, dv_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "hsNO9nnXQBvP",
    "outputId": "1626def6-94c7-4a87-9447-d939f827c8eb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(model_loss_record, title='deep model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8cTuQjQQOon",
    "outputId": "6bc5de07-4c5a-4e87-9ae3-d09f539c5f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to pred.csv\n"
     ]
    }
   ],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    print('Saving results to {}'.format(file))\n",
    "    with open(file, 'w', newline='') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['PerNo', 'PerStatus'])\n",
    "        for i, p in preds:\n",
    "            writer.writerow([i,p])\n",
    "model.eval() # set the model to evaluation mode\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs, idx in tt_set:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        preds.extend([[int(idx[i]), int(pred[i])]for i in range(pred.shape[0])])\n",
    "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML2021Spring - HW1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
